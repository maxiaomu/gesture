{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gcForest Aligorithm\n",
    "The gcForest algorithm was suggested in Zhou and Feng 2017 ([https://arxiv.org/abs/1702.08835 , refer for this paper for technical details](https://arxiv.org/abs/1702.08835 , refer for this paper for technical details)) and I provide here a python3 implementation of this algorithm.<br/>I chose to adopt the scikit-learn syntax for ease of use and hereafter I present how it can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GCForest import gcForest\n",
    "from sklearn.datasets import load_iris, load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting, a word about sizes.\n",
    "*Note* : I recommend the reader to look at this section with the original paper next to the computer to see what I am talking about.\n",
    "The main technical problem in the present gcForest implementation so far is the memory usage when slicing the input data. A naive calculation can actually give you an idea of the number and sizes of objects the algorithm will be dealing with.\n",
    "Starting with a dataset of $N$ samples of size $[l,L]$ and with $C$ classes, the initial \"size\" is:\n",
    "$S_{D} = N.l.L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing Step\n",
    "If my window is of size $[w_l,w_L]$ and the chosen stride are $[s_l,sL]$ then the number of slices per sample is :\n",
    "\n",
    "$n{slices} = \\left(\\frac{l-w_l}{s_l}+1\\right)\\left(\\frac{L-w_L}{s_L}+1\\right)$\n",
    "\n",
    "Obviously the size of slice is $w_l.wL$ hence the total size of the sliced data set is :\n",
    "\n",
    "$S{sliced} = N.w_l.w_L.\\left(\\frac{l-w_l}{s_l}+1\\right)\\left(\\frac{L-w_L}{sL}+1\\right)$\n",
    "This is when the memory consumption is its peak maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Vector after Multi-Grain Scanning\n",
    "Now all slices are fed to the random forest to generate class vectors. The number of class vector per random forest per window per sample is simply equal to the number of slices given to the random forest $n{cv}(w) = n{slices}(w)$. Hence, if we have $N{RF}$ random forest per window the size of a class vector is (recall we have $N$ samples and $C$ classes):\n",
    "\n",
    "$S{cv}(w) = N.n{cv}(w).N{RF}.C$\n",
    "\n",
    "And finally the total size of the Multi-Grain Scanning output will be:\n",
    "\n",
    "$S{mgs} = N.\\sum{w} N{RF}.C.n_{cv}(w)$\n",
    "\n",
    "This short calculation is just meant to give you an idea of the data processing during the Multi-Grain Scanning phase. The actual memory consumption depends on the format given (aka float, int, double, etc.) and it might be worth looking at it carefully when dealing with large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris example\n",
    "\n",
    "The iris data set is actually not a very good example as the gcForest algorithm is better suited for time series and images where informations can be found at different scales in one sample.\n",
    "Nonetheless it is still an easy way to test the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4) (150,)\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print(X.shape,y.shape)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing Sequence...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9\n"
     ]
    }
   ],
   "source": [
    "gcf = gcForest(shape_1X=4, window=2,tolerance=0.0)\n",
    "gcf.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now checking the prediction for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing Sequence...\n",
      "[2 1 2 1 0 1 1 0 0 0 0 1 0 1 1 0 1 0 0 0 2 0 0 1 2 1 2 2 2 1 0 0 2 2 2 1 1\n",
      " 1 2 0 2 1 0 0 0 1 0 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "pred_X = gcf.predict(X_te)\n",
    "print(pred_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcForest accuarcy : 1.0\n"
     ]
    }
   ],
   "source": [
    "# evaluating accuracy\n",
    "accuarcy = accuracy_score(y_true=y_te, y_pred=pred_X)\n",
    "print('gcForest accuarcy : {}'.format(accuarcy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits Example\n",
    "A much better example is the digits data set containing images of hand written digits. The scikit data set can be viewed as a mini-MNIST for training purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n",
      "(1078, 64) (719, 64)\n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "print(X.shape,y.shape)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.4)\n",
    "print(X_tr.shape,X_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... taining gcForest ... (can take some time...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing Images...\n",
      "Training MGS Random Forests...\n",
      "Slicing Images...\n",
      "Training MGS Random Forests...\n",
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9861111111111112\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9907407407407407\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.9907407407407407\n"
     ]
    }
   ],
   "source": [
    "gcf = gcForest(shape_1X=[8,8], window=[4,6], tolerance=0.0, min_samples_mgs=10, min_samples_cascade=7)\n",
    "gcf.fit(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and predicting classes .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing Images...\n",
      "Slicing Images...\n",
      "[5 9 6 5 9 0 6 2 4 9 9 7 1 4 0 4 5 6 6 6 5 4 1 3 9 3 0 3 6 6 2 8 9 8 4 6 2\n",
      " 5 7 5 3 7 8 9 8 8 6 3 1 7 8 4 9 5 7 3 2 5 6 3 6 6 9 2 5 6 1 0 6 6 9 5 1 8\n",
      " 1 8 9 8 9 0 0 0 0 7 1 4 9 9 6 1 4 4 2 1 2 0 8 8 7 6 9 4 7 8 6 0 8 5 5 0 9\n",
      " 7 1 7 5 4 8 2 2 6 4 0 8 1 6 3 7 2 2 3 6 8 0 9 0 0 8 8 6 3 4 2 9 8 0 9 4 8\n",
      " 4 5 1 1 9 9 7 1 2 0 6 0 2 9 9 0 0 1 3 4 9 5 4 6 7 8 1 6 5 8 7 6 5 7 1 1 2\n",
      " 6 3 1 7 3 2 7 3 3 6 5 2 3 7 6 9 6 6 7 7 0 5 9 5 6 0 8 2 0 3 1 6 1 6 8 9 3\n",
      " 3 8 6 3 4 8 8 7 7 2 0 1 6 3 6 1 1 0 0 3 4 7 2 8 6 5 9 0 4 1 6 7 1 3 2 2 3\n",
      " 2 0 0 4 6 1 6 3 0 5 5 5 2 7 2 2 0 8 4 7 4 4 2 4 8 4 0 9 6 3 4 2 8 9 2 0 0\n",
      " 0 0 0 1 6 8 5 2 3 1 2 4 9 0 1 6 5 8 3 4 4 8 5 2 6 6 3 5 1 4 7 3 7 9 0 7 4\n",
      " 5 1 9 8 9 7 4 6 1 4 2 4 2 6 1 6 1 9 4 0 0 7 1 7 1 0 9 2 8 3 6 4 0 7 3 0 6\n",
      " 8 9 7 0 9 6 0 1 1 7 6 9 2 5 8 4 9 3 4 4 5 7 0 5 1 0 5 5 1 6 6 8 7 4 3 4 2\n",
      " 5 7 2 5 5 4 1 8 7 4 2 5 7 9 9 8 3 0 4 1 4 5 6 3 5 7 1 5 8 3 7 7 6 5 2 5 1\n",
      " 7 7 7 7 7 9 4 1 6 7 7 6 1 5 7 9 9 4 9 1 0 0 6 4 7 8 6 3 1 5 1 3 7 1 9 8 9\n",
      " 1 0 5 9 6 3 0 0 7 9 5 4 6 1 9 6 2 7 4 8 3 1 2 6 3 3 6 8 7 8 0 4 7 8 4 3 0\n",
      " 6 0 2 7 2 0 6 1 8 2 0 5 5 1 5 2 8 9 2 9 5 4 5 4 5 3 7 6 1 3 2 6 8 4 1 2 2\n",
      " 5 0 2 6 3 6 9 8 5 5 5 1 3 7 9 2 1 4 0 7 0 6 1 1 2 2 8 5 0 9 8 7 9 1 4 5 0\n",
      " 7 0 0 9 1 1 5 8 0 7 7 2 9 3 2 5 6 4 7 8 1 3 2 6 6 5 7 4 6 2 9 4 1 7 5 1 3\n",
      " 1 9 2 3 7 9 2 5 7 6 0 6 2 4 8 2 3 1 0 3 9 0 2 1 9 8 1 5 4 7 1 0 6 1 8 8 5\n",
      " 2 8 9 7 6 0 6 9 9 0 6 0 7 5 0 2 3 4 8 1 1 9 3 4 1 1 5 8 4 5 4 9 4 2 5 9 4\n",
      " 2 0 0 0 4 9 3 4 0 6 7 3 3 7 4 4]\n"
     ]
    }
   ],
   "source": [
    "pred_X = gcf.predict(X_te)\n",
    "print(pred_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcForest accuracy : 0.980528511821975\n"
     ]
    }
   ],
   "source": [
    "# evaluating accuracy\n",
    "accuracy = accuracy_score(y_true=y_te, y_pred=pred_X)\n",
    "print('gcForest accuracy : {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Models to Disk\n",
    "\n",
    "You probably don't want to re-train your classifier every day especially if you're using it on large data sets. Fortunately there is a very easy way to save and load models to disk using `sklearn.externals.joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gcf_model.sav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(gcf, 'gcf_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcf = joblib.load('gcf_model.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using mg-scanning and cascade_forest Sperately\n",
    "As the Multi-Grain scanning and the cascade forest modules are quite independent it is possible to use them seperately.<br>If a target y is given the code automaticcaly use it for training otherwise it recalls the last trained Random Forests to slice the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing Images...\n",
      "Training MGS Random Forests...\n",
      "(1078, 320)\n",
      "(1078, 64)\n"
     ]
    }
   ],
   "source": [
    "gcf = gcForest(shape_1X=[8,8],window=5,min_samples_mgs=10,min_samples_cascade=7)\n",
    "X_tr_mgs = gcf.mg_scanning(X_tr, y_tr)\n",
    "print(X_tr_mgs.shape)\n",
    "print(X_tr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing Images...\n"
     ]
    }
   ],
   "source": [
    "X_te_mgs = gcf.mg_scanning(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to use the mg_scanning output as input for cascade forests using different parameters. Note that the cascade forest module does not directly return predictions but probability predictions from each Random Forest in the last layer of the cascade. Hence the need to first take the mean of the output and then find the max."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9814814814814815\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9814814814814815\n"
     ]
    }
   ],
   "source": [
    "gcf = gcForest(tolerance=0.0, min_samples_mgs=10, min_samples_cascade=7)\n",
    "_ = gcf.cascade_forest(X_tr_mgs,y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97496522948539643"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba = gcf.cascade_forest(X_te_mgs)\n",
    "#print(X_te_mgs.shape)\n",
    "#print(pred_proba[1])\n",
    "tmp = np.mean(pred_proba,axis=0)\n",
    "#print(tmp.shape)\n",
    "preds = np.argmax(tmp,axis=1)\n",
    "accuracy_score(y_true=y_te, y_pred=preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9907407407407407\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9953703703703703\n",
      "Adding/Training Layer, n_layer=3\n",
      "Layer validation accuracy = 0.9953703703703703\n"
     ]
    }
   ],
   "source": [
    "gcf = gcForest(tolerance=0.0, min_samples_mgs=20, min_samples_cascade=10)\n",
    "_ = gcf.cascade_forest(X_tr_mgs, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98191933240611962"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba = gcf.cascade_forest(X_te_mgs)\n",
    "tmp = np.mean(pred_proba, axis=0)\n",
    "preds = np.argmax(tmp, axis=1)\n",
    "accuracy_score(y_true=y_te, y_pred=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skipping mg_scanning\n",
    "\n",
    "It is also possible to directly use the cascade forest and skip the multi grain scanning step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding/Training Layer, n_layer=1\n",
      "Layer validation accuracy = 0.9675925925925926\n",
      "Adding/Training Layer, n_layer=2\n",
      "Layer validation accuracy = 0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "gcf = gcForest(tolerance=0.0, min_samples_cascade=20)\n",
    "_ = gcf.cascade_forest(X_tr, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95132127955493739"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba = gcf.cascade_forest(X_te)\n",
    "tmp = np.mean(pred_proba, axis=0)\n",
    "preds = np.argmax(tmp, axis=1)\n",
    "accuracy_score(y_true=y_te, y_pred=preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **window的选择决定了不同的粒度，如5，为只用5的窗口去滑动，而[4,5]则是用4和5分别滑动然后append**\n",
    "* **参考[股指期货涨跌](https://blog.csdn.net/woddle/article/details/71122698)**\n",
    "## gcForest参数说明\n",
    "\n",
    "**shape_1X: **\n",
    "单个样本元素的形状[n_lines，n_cols]。 调用mg_scanning时需要！对于序列数据，可以给出单个int。\n",
    "\n",
    "**n_mgsRFtree: **\n",
    "多粒度扫描期间随机森林中的树木数量。\n",
    "\n",
    "**window：int（default = None） **\n",
    "多粒度扫描期间使用的窗口大小列表。如果“无”，则不进行切片。\n",
    "\n",
    "**stride：int（default = 1） **\n",
    "切片数据时使用的步骤。\n",
    "\n",
    "**cascade_test_size：float或int（default = 0.2）** \n",
    "级联训练集分裂的分数或绝对数。\n",
    "\n",
    "**n_cascadeRF：int（default = 2） **\n",
    "级联层中随机森林的数量,对于每个伪随机森林，创建完整的随机森林，因此一层中随机森林的总数将为2 * n_cascadeRF。\n",
    "\n",
    "**n_cascadeRFtree：int（default = 101） **\n",
    "级联层中单个随机森林中的树数。\n",
    "\n",
    "**min_samples_mgs：float或int（default = 0.1） **\n",
    "节点中执行拆分的最小样本数 在多粒度扫描随机森林训练期间。 如果int number_of_samples = int。 如果float，min_samples表示要考虑的初始n_samples的分数。\n",
    "\n",
    "**min_samples_cascade：float或int（default = 0.1） **\n",
    "节点中执行拆分的最小样本数 在级联随机森林训练期间。 如果int number_of_samples = int。 如果float，min_samples表示要考虑的初始n_samples的分数。\n",
    "\n",
    "**cascade_layer：int（default = np.inf） **\n",
    "允许的最大级联级数。 有用的限制级联的结构。\n",
    "\n",
    "**tolerance：float（default= 0.0） **\n",
    "联生长的精度差,整个级联的性能将在验证集上进行估计， 如果没有显着的性能增益，训练过程将终止\n",
    "\n",
    "**n_jobs：int（default = 1） **\n",
    "任意随机森林适合并预测的并行运行的工作数量。 如果为-1，则将作业数设置为核心数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
